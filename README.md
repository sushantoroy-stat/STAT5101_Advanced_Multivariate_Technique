## ðŸ“‘ Table of Contents
- [Introduction](#introduction)
- [Tools Used Here](#tools-used-here)
- [L1 Principal Component Analysis](#l1-principal-component-analysis)
- [L2 Covariance Matrix](#l2-covariance-matrix)
- [L3 Factorial Analysis](#l3-factorial-analysis)

---

- ## Introduction
This repository showcases my academic projects focused on statistical analysis and data science. It includes in-depth implementations of techniques such as **Factor Analysis, Principal Component Analysis (PCA), Clustering algorithms (including DBSCAN), Multidimensional Scaling (MDS)** and other advanced analytical methods. All projects are developed and demonstrated using **Jupyter Notebook (Python)** with an emphasis on both theoretical understanding and practical application through real-world or sample academic datasets.

- ## Tools Used Here
- **Jupyter Notebook** â€“ Interactive environment for writing and running Python code.
- **NumPy** â€“ Numerical computing and array operations.
- **pandas** â€“ Data manipulation and analysis.
- **scikit-learn** â€“ Machine learning and statistical modeling, including:
- `PCA` for Principal Component Analysis
- `FactorAnalysis` for Factor Analysis
- `KMeans` for K-Means Clustering
- `DBSCAN` for Density-Based Spatial Clustering
- `matplotlib` â€“ Data visualization.
- `seaborn` â€“ Statistical data visualization with enhanced aesthetics.

- ## L1 Principal Component Analysis
Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction â€” that means it reduces the number of variables (features) in a dataset while keeping as much important information (variance) as possible.

âš™ How PCA Works (Steps)
1. Standardize the data (so all features are on the same scale).
2. Compute the covariance matrix of the features.
3. Find the eigenvalues and eigenvectors of the covariance matrix.
4. Sort eigenvalues in descending order â€” higher eigenvalue â†’ more variance explained.
5. Select top k eigenvectors to form new feature space (k < original number of features).
6. Transform original data into this reduced feature space.

## From this Analysis


<img width="780" height="667" alt="image" src="https://github.com/user-attachments/assets/9d10885f-888e-4699-b788-b468d483d865" />









- ## L2 Covariance Matrix
(Your covariance matrix content here)

- ## L3 Factorial Analysis
(Your factorial analysis content here)
